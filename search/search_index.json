{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Cerberus Documentation Quick introduction ? But already one in I. Overview Table of Content Overview a. What is Cerberus-Sandbox ? b. How it works ? c. Comparison Development Procedures a. Updating the docs b. Coding Standards c. Git Workflow d. Module Development Guide Installation a. Foreword b. Requirements c. NIDS d. VMI Internal Design a. Index","title":"Welcome to Cerberus Documentation"},{"location":"#welcome-to-cerberus-documentation","text":"Quick introduction ? But already one in I. Overview Table of Content Overview a. What is Cerberus-Sandbox ? b. How it works ? c. Comparison Development Procedures a. Updating the docs b. Coding Standards c. Git Workflow d. Module Development Guide Installation a. Foreword b. Requirements c. NIDS d. VMI Internal Design a. Index","title":"Welcome to Cerberus Documentation"},{"location":"I.%20Overview/1.%20What%20is%20Cerberus-Sandbox/","text":"What is Cerberus-Sandbox ? Cerberus is an automated malware analysis environment. The purpose is to help Analysist during the first repetitive steps when they find a new malware. Cerberus is an open source software, he was built like a white box, nothing is hide from the user. The sandbox can be deployed locally and support an offline uses. Why we made this sandbox ? This sandbox was made because there are lack of white sandbox. When you put a malware sample in sandbox like Cuckoo, Joe, Hybrid-Analysis or any-run, you dont know what they are doing with it. Cerberus allow you to keep malware for personal purpose. Use Cases It can be used to analyse: - Generic Windows executables - DLL files - PCAP It's modularity allow you to do what ever you want to achieve with Cerberus. Architecture The following picture explains Cerberus main architecture: Obtaining Cerberus Cerberus can be download from our official git repository .","title":"1. What is Cerberus Sandbox"},{"location":"I.%20Overview/1.%20What%20is%20Cerberus-Sandbox/#what-is-cerberus-sandbox","text":"Cerberus is an automated malware analysis environment. The purpose is to help Analysist during the first repetitive steps when they find a new malware. Cerberus is an open source software, he was built like a white box, nothing is hide from the user. The sandbox can be deployed locally and support an offline uses.","title":"What is Cerberus-Sandbox ?"},{"location":"I.%20Overview/1.%20What%20is%20Cerberus-Sandbox/#why-we-made-this-sandbox","text":"This sandbox was made because there are lack of white sandbox. When you put a malware sample in sandbox like Cuckoo, Joe, Hybrid-Analysis or any-run, you dont know what they are doing with it. Cerberus allow you to keep malware for personal purpose.","title":"Why we made this sandbox ?"},{"location":"I.%20Overview/1.%20What%20is%20Cerberus-Sandbox/#use-cases","text":"It can be used to analyse: - Generic Windows executables - DLL files - PCAP It's modularity allow you to do what ever you want to achieve with Cerberus.","title":"Use Cases"},{"location":"I.%20Overview/1.%20What%20is%20Cerberus-Sandbox/#architecture","text":"The following picture explains Cerberus main architecture:","title":"Architecture"},{"location":"I.%20Overview/1.%20What%20is%20Cerberus-Sandbox/#obtaining-cerberus","text":"Cerberus can be download from our official git repository .","title":"Obtaining Cerberus"},{"location":"I.%20Overview/2.%20How%20it%20works/","text":"","title":"2. How it works"},{"location":"I.%20Overview/3.%20Comparison/","text":"3. Comparison TODO","title":"3. Comparison"},{"location":"I.%20Overview/3.%20Comparison/#3-comparison","text":"TODO","title":"3. Comparison"},{"location":"II.%20Development%20Procedures/1.%20Updating%20the%20docs/","text":"Updating the docs This online documentation is generated using mkdocs and is hosted on a Github Pages . The following procedure explains how to build this documentation on your own system, and how to upgrade the live one. Local Setup In order to build the local version of this documentation, the mkdocs package is required: $ pip install mkdocs $ mkdocs -V mkdocs, version 1.1.2 from /usr/lib/python3.8/site-packages/mkdocs (Python 3.8) To get the last version of the documentation's file, clone the Cerberus Sandbox public GitHub repository in one of your local folder: $ git clone https://github.com/Cerberus-Sandbox/cerberus-sandbox.github.io The GitHub repository is composed of two branches: main and master . The first one holds the raw mkdocs files, and the second one is the compiled version of the website used by GitHub to host the live instance. You should never edit the master branch by hand. Even in a local setup. Now that you hold a local copy of the documentation website, simply run mkdocs build to generate the compiled HTML pages. The entrypoint of the local setup is located in site/index.html . If you want to edit the live content, or if you need to add a new page, everything will be done in the docs/ folder. A typical architecture can be resumed by the following: docs/ \u251c\u2500\u2500 I. Subsection n\u00b01 \u2502 \u251c\u2500\u2500 1. Doc 1.md \u2502 \u251c\u2500\u2500 2. Doc 2.md \u2502 \u2514\u2500\u2500 3. Doc 3.md \u2514\u2500\u2500 II. Subsection n\u00b02 Each folder holds a subsection of the documentation, and each markedown file stores the actual content of the page. You can refer to the following cheatsheet to edit your markedown file: www.markdownguide.org/cheat-sheet/ Upgrading the live version Whenever the local content is ready to go, you can use the automatic script ship-to-prod.sh to push the new content on GitHub and rebuild the mkdocs instance, or choose to do it by hands. The script is located at the root folder of the cerberus-sandbox repository. You can choose to pass a short description of your modification to the script, as a string value, for a better understanding of each commit: $ chmod +x ./ship-to-prod.sh $ ./ship-to-prod.sh \"index update + typo\" And that's it, shortly after that, the upgraded version of the documentation website should be up and running. For the manual process, two steps are needed: First, the mkdocs sources need to be saved in the main branch: $ git pull $ git checkout main $ git add docs/* $ git add mkdocs.yml $ git commit -m \"your commit message\" $ git push If you don't push to the main branch, the next modifications will erase yours, and nobody will be able to track and download your modifications. Then, the mkdocs sources can be compiled and shiped to the github master branch: $ mkdocs gh-deploy --force --remote-branch master And that's it. Remember that you'll have to wait several minutes to see your updates on the live page ! Pages layout Every entry in the Documentation need to follow the same layout. The goal is to keep everything consistent with a set of basics rules for the contributors. This section is basically a list of rules to follow in order to fully integrate your page in this project. A page need to have one (and only one) title, and that title should match the name of the file, even if it contain some special characters or some spaces. ## This is a title It match the file 'This is a title.md' When possible, split your page with sub-sections. It will help the navigation in the side menu, and a quick way to find information in a large page. ### This is a sub-section At the beginning of the page, a short description should be made. This can be a sum-up of the page, or a way to identify what the reader will learn. When the name of a script, a program file, or a path is used, put it in back-quotes: `python` is the best interpreted language Do not write code outside the markedown code blocks: ```python print(\"this is code\") ``` Be as precise as possible when writting about practical technical stuff, and as theoretical as possible when writting about some high-level concepts. Generaly speaking, thoses two part should be keept in separated pages in different sections. But be carrefull not to be too precise. This is a documentation about the Cerberus-Sandbox , we don't need to re-write technical documentation about already-existing tools. Even in a technical installation process, don't blindly list commands to copy and paste. Explain everything that is relevant. Always use relative path, except if the path that you are talking about is standard for everyone. Do not use any hardcoded IP or username in your pages. Do not write down any personnal credentials or API keys.","title":"1. Updating the docs"},{"location":"II.%20Development%20Procedures/1.%20Updating%20the%20docs/#updating-the-docs","text":"This online documentation is generated using mkdocs and is hosted on a Github Pages . The following procedure explains how to build this documentation on your own system, and how to upgrade the live one.","title":"Updating the docs"},{"location":"II.%20Development%20Procedures/1.%20Updating%20the%20docs/#local-setup","text":"In order to build the local version of this documentation, the mkdocs package is required: $ pip install mkdocs $ mkdocs -V mkdocs, version 1.1.2 from /usr/lib/python3.8/site-packages/mkdocs (Python 3.8) To get the last version of the documentation's file, clone the Cerberus Sandbox public GitHub repository in one of your local folder: $ git clone https://github.com/Cerberus-Sandbox/cerberus-sandbox.github.io The GitHub repository is composed of two branches: main and master . The first one holds the raw mkdocs files, and the second one is the compiled version of the website used by GitHub to host the live instance. You should never edit the master branch by hand. Even in a local setup. Now that you hold a local copy of the documentation website, simply run mkdocs build to generate the compiled HTML pages. The entrypoint of the local setup is located in site/index.html . If you want to edit the live content, or if you need to add a new page, everything will be done in the docs/ folder. A typical architecture can be resumed by the following: docs/ \u251c\u2500\u2500 I. Subsection n\u00b01 \u2502 \u251c\u2500\u2500 1. Doc 1.md \u2502 \u251c\u2500\u2500 2. Doc 2.md \u2502 \u2514\u2500\u2500 3. Doc 3.md \u2514\u2500\u2500 II. Subsection n\u00b02 Each folder holds a subsection of the documentation, and each markedown file stores the actual content of the page. You can refer to the following cheatsheet to edit your markedown file: www.markdownguide.org/cheat-sheet/","title":"Local Setup"},{"location":"II.%20Development%20Procedures/1.%20Updating%20the%20docs/#upgrading-the-live-version","text":"Whenever the local content is ready to go, you can use the automatic script ship-to-prod.sh to push the new content on GitHub and rebuild the mkdocs instance, or choose to do it by hands. The script is located at the root folder of the cerberus-sandbox repository. You can choose to pass a short description of your modification to the script, as a string value, for a better understanding of each commit: $ chmod +x ./ship-to-prod.sh $ ./ship-to-prod.sh \"index update + typo\" And that's it, shortly after that, the upgraded version of the documentation website should be up and running. For the manual process, two steps are needed: First, the mkdocs sources need to be saved in the main branch: $ git pull $ git checkout main $ git add docs/* $ git add mkdocs.yml $ git commit -m \"your commit message\" $ git push If you don't push to the main branch, the next modifications will erase yours, and nobody will be able to track and download your modifications. Then, the mkdocs sources can be compiled and shiped to the github master branch: $ mkdocs gh-deploy --force --remote-branch master And that's it. Remember that you'll have to wait several minutes to see your updates on the live page !","title":"Upgrading the live version"},{"location":"II.%20Development%20Procedures/1.%20Updating%20the%20docs/#pages-layout","text":"Every entry in the Documentation need to follow the same layout. The goal is to keep everything consistent with a set of basics rules for the contributors. This section is basically a list of rules to follow in order to fully integrate your page in this project. A page need to have one (and only one) title, and that title should match the name of the file, even if it contain some special characters or some spaces. ## This is a title It match the file 'This is a title.md' When possible, split your page with sub-sections. It will help the navigation in the side menu, and a quick way to find information in a large page. ### This is a sub-section At the beginning of the page, a short description should be made. This can be a sum-up of the page, or a way to identify what the reader will learn. When the name of a script, a program file, or a path is used, put it in back-quotes: `python` is the best interpreted language Do not write code outside the markedown code blocks: ```python print(\"this is code\") ``` Be as precise as possible when writting about practical technical stuff, and as theoretical as possible when writting about some high-level concepts. Generaly speaking, thoses two part should be keept in separated pages in different sections. But be carrefull not to be too precise. This is a documentation about the Cerberus-Sandbox , we don't need to re-write technical documentation about already-existing tools. Even in a technical installation process, don't blindly list commands to copy and paste. Explain everything that is relevant. Always use relative path, except if the path that you are talking about is standard for everyone. Do not use any hardcoded IP or username in your pages. Do not write down any personnal credentials or API keys.","title":"Pages layout"},{"location":"II.%20Development%20Procedures/2.%20Coding%20Standards/","text":"Coding Standards This page is a list of standards that every scripts or programs in the cerberus-sandbox project must match. If a program file does not follow thoses standards, upgrade them. The goal is to bind every file of the project in a global scope, with the same high-level sythax. Navigating through the source code of the sandbox should be as user-friendly as possible, and the code and it's comments must speak for themselves. It will help to integrate new functionality latter, keep everything clear, and eventually to maintain the codebase. Whenever a new script file is added to the project, it name must be the concatenation of what it is doing, keeping only the keywords. Instead of adding spaces, the first letter of each word will be in the uppercase format. i.e: StaticEngine.py , Disassembler.py , PeParser.py or FileDriver.py . Python3 should be the only autorized version of Python. The first line of the script must indicate the encoding. By default, we'll use the UTF-8 encoding. # coding: utf-8 After the encoding, the next 3 lines should be used as a header to indicate the date of creation of the original file, the name of the author and a quick description of what the script is doing, or how to use it. # 17/10/2020 # HomardBoy # Low-level linear disassembler based on the Capstone library. Everything related to the python code should match the PEP-8 standards. To quickly check if a file is following the PEP-8 rules, you can use the pep8 tool: $ pip install pep8 $ pep8 --first Disassembler.py Disassembler.py:222:34: W602 deprecated form of raising exception Disassembler.py:347:31: E211 whitespace before '(' Apart from the PEP-8 standard, you are encouraged to follow those additional rules: For performance, try to only import the Python's modules that you need, not the full library ( never import the full library as from lib import * ): from lib import function For performance reason, string formating should not be done using concatenation of variables and strings, nor by using format strings. Instead, use f-strings. They are faster to load, and very clear to read: name = \"cerberus\" f\"The best sandbox is {name}-sandbox.\" Logs messages should use the Python logger objects. Don't print anything, log it. If your script or function support this usecase, try to pass the log level as an argument. Always keep your logs message as clear and minimal as possible. import logging logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG) logging.debug('I am here for information only') logging.info('I am used to track how the program is going') logging.warning('Something might be broken') logging.error('Something is broken') logging.critical('Everything is broken') Use f-strings inside your loggers objects: logging.error('the file {filename} was not found') Configure the logs of your application to be redirect to a static logfile, and to stdout (you can skip the stdout redirect part if you don't need to; You are in fact advise not to print anything on stdout): logging.basicConfig(filename='out.log', format='[%(asctime)s][%(name)s][%(levelname)s] %(message)s', filemode='w+') logger = logging.getLogger('Application name') # Name of the application that is going to generate logs logger.setLevel('DEBUG') # The static log file should have the higher log level console = logging.StreamHandler() # 'console' identify the logs that should be print to stdout console.setLevel(logging.getLevelName(args.log)) # The console logging is choosen by the user (without affecting the static log file level) formatter = logging.Formatter('[%(asctime)s][%(name)s][%(levelname)s] %(message)s') console.setFormatter(formatter) # Set the format of the stdout logs logger.addHandler(console) # The static file will also get a copy of everything that is print to the console The logger output should be redirect by default to /var/log/cerberus.log : parser.add_argument(\"-o\", \"--output\", help = \"Log file. Default=/var/log/cerberus.log\", required = False, default = \"/var/log/cerberus.log\") logging.basicConfig(filename=args.output, ...) The logs format should match this one (but you can add additionnal fields inside if you need to be more precise): format='[%(asctime)s][%(name)s][%(levelname)s] %(message)s' Functions name should describe the global job of the function, with the same construct as the filename (using uppercase letter for each new word, and without any spaces). The first letter should be lowercase: def addWinApiArguments(): return Variables names must be as clear as possible, without uppercase, using underscore as a \"space\" character: user_input = input(\"How are you ?\") Global variables should be avoided as much as possible. Comment your functions using the Sphinx standard. def disassemble(entry_point, end_point, f): '''Returns the assembly code for a bunch of given bytes :param entry_point: start address of the function to disassemble. Only use for RVA. :type entry_point: integer :param end_point: address of the last instruction of the function. Only used to check if the result is still inside the CODE section. :type end_point: integer :param f: byte representation of the function to disassemble. :type f: bytes-array :returns: Disassembly version of the target function. :rtype: list of strings ''' Try to use multi-thread as much as you can when your script is doing two differents things at the same time. Each part of the sandbox should be design to be as less time-consuming as possible since the sandbox cannot affort to make the end user hang for too long. Performance is key. Use OOP when needed, but don't use it for anything when it doesn't make sense. If what you are writting is going to be used in multiple instances with different values (exception given for functions with kilometers longs list of arguments), then go for OOP. If not, build simple functions or more linear code. Use Python3 build-in functions when you can. It will improve the execution time of your scripts.","title":"2. Coding Standards"},{"location":"II.%20Development%20Procedures/2.%20Coding%20Standards/#coding-standards","text":"This page is a list of standards that every scripts or programs in the cerberus-sandbox project must match. If a program file does not follow thoses standards, upgrade them. The goal is to bind every file of the project in a global scope, with the same high-level sythax. Navigating through the source code of the sandbox should be as user-friendly as possible, and the code and it's comments must speak for themselves. It will help to integrate new functionality latter, keep everything clear, and eventually to maintain the codebase. Whenever a new script file is added to the project, it name must be the concatenation of what it is doing, keeping only the keywords. Instead of adding spaces, the first letter of each word will be in the uppercase format. i.e: StaticEngine.py , Disassembler.py , PeParser.py or FileDriver.py . Python3 should be the only autorized version of Python. The first line of the script must indicate the encoding. By default, we'll use the UTF-8 encoding. # coding: utf-8 After the encoding, the next 3 lines should be used as a header to indicate the date of creation of the original file, the name of the author and a quick description of what the script is doing, or how to use it. # 17/10/2020 # HomardBoy # Low-level linear disassembler based on the Capstone library. Everything related to the python code should match the PEP-8 standards. To quickly check if a file is following the PEP-8 rules, you can use the pep8 tool: $ pip install pep8 $ pep8 --first Disassembler.py Disassembler.py:222:34: W602 deprecated form of raising exception Disassembler.py:347:31: E211 whitespace before '(' Apart from the PEP-8 standard, you are encouraged to follow those additional rules: For performance, try to only import the Python's modules that you need, not the full library ( never import the full library as from lib import * ): from lib import function For performance reason, string formating should not be done using concatenation of variables and strings, nor by using format strings. Instead, use f-strings. They are faster to load, and very clear to read: name = \"cerberus\" f\"The best sandbox is {name}-sandbox.\" Logs messages should use the Python logger objects. Don't print anything, log it. If your script or function support this usecase, try to pass the log level as an argument. Always keep your logs message as clear and minimal as possible. import logging logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG) logging.debug('I am here for information only') logging.info('I am used to track how the program is going') logging.warning('Something might be broken') logging.error('Something is broken') logging.critical('Everything is broken') Use f-strings inside your loggers objects: logging.error('the file {filename} was not found') Configure the logs of your application to be redirect to a static logfile, and to stdout (you can skip the stdout redirect part if you don't need to; You are in fact advise not to print anything on stdout): logging.basicConfig(filename='out.log', format='[%(asctime)s][%(name)s][%(levelname)s] %(message)s', filemode='w+') logger = logging.getLogger('Application name') # Name of the application that is going to generate logs logger.setLevel('DEBUG') # The static log file should have the higher log level console = logging.StreamHandler() # 'console' identify the logs that should be print to stdout console.setLevel(logging.getLevelName(args.log)) # The console logging is choosen by the user (without affecting the static log file level) formatter = logging.Formatter('[%(asctime)s][%(name)s][%(levelname)s] %(message)s') console.setFormatter(formatter) # Set the format of the stdout logs logger.addHandler(console) # The static file will also get a copy of everything that is print to the console The logger output should be redirect by default to /var/log/cerberus.log : parser.add_argument(\"-o\", \"--output\", help = \"Log file. Default=/var/log/cerberus.log\", required = False, default = \"/var/log/cerberus.log\") logging.basicConfig(filename=args.output, ...) The logs format should match this one (but you can add additionnal fields inside if you need to be more precise): format='[%(asctime)s][%(name)s][%(levelname)s] %(message)s' Functions name should describe the global job of the function, with the same construct as the filename (using uppercase letter for each new word, and without any spaces). The first letter should be lowercase: def addWinApiArguments(): return Variables names must be as clear as possible, without uppercase, using underscore as a \"space\" character: user_input = input(\"How are you ?\") Global variables should be avoided as much as possible. Comment your functions using the Sphinx standard. def disassemble(entry_point, end_point, f): '''Returns the assembly code for a bunch of given bytes :param entry_point: start address of the function to disassemble. Only use for RVA. :type entry_point: integer :param end_point: address of the last instruction of the function. Only used to check if the result is still inside the CODE section. :type end_point: integer :param f: byte representation of the function to disassemble. :type f: bytes-array :returns: Disassembly version of the target function. :rtype: list of strings ''' Try to use multi-thread as much as you can when your script is doing two differents things at the same time. Each part of the sandbox should be design to be as less time-consuming as possible since the sandbox cannot affort to make the end user hang for too long. Performance is key. Use OOP when needed, but don't use it for anything when it doesn't make sense. If what you are writting is going to be used in multiple instances with different values (exception given for functions with kilometers longs list of arguments), then go for OOP. If not, build simple functions or more linear code. Use Python3 build-in functions when you can. It will improve the execution time of your scripts.","title":"Coding Standards"},{"location":"II.%20Development%20Procedures/3.%20Git%20Workflow/","text":"","title":"3. Git Workflow"},{"location":"II.%20Development%20Procedures/4.%20Module%20Development%20Guide/","text":"Module Development Guide This short guide aim to help developers to write and integrate new functionalities in the Cerberus-Sandbox. In Cerberus-Sandbox, every functionnal piece of code can be describe as a module. A module can work as a standalone tool, but can also be easily integrated in the general workflow of the sandbox. The following will give you an insight on what's a module, how to make it works, and how to integrate it into Cerberus. General Layout Modules are stored as Python scripts under cerberus/core/static_engine/Modules/ . By keeping everything in a modular architecture, we aim to encourage developers to add custom functionnalities that match their needs, without having to understand the whole code-base of the project. It is also easier for the inital development stage and for further official upgrades, as we don't need to deal with already existing works and only focus on the core of the functionnalities that we want to add. It also gave the ability to easily write new connector with already-existing products, wich is something we want to push forward. Orchestrator The script cerberus/core/static_engine/main.py is in charge of the orchestration of the modules. This script is the one that gather, parse and execute modules. If a new Python script is simply dropped in the modules folder, without any tweaks to make it works with the so called orchestrator, it will simply be ignored at runtime. If your module is malformated, this is the script that will tell you why and where. Don't make modification to the core of this script in order to match your needs ! If you have to, it means that your module is not well-written. Module layout Each module must have a header with the following items: PRIORITY = INPUT = [''] OUTPUT = [''] RETURNS = [''] RETURN_CODE = { 0 : \"Something\" } Priority The PRIORITY keyword is used to tell in which order the modules must be launched. Priority numbers don't have to be contiguous. If you want to write several modules, you can reserve an arbitray range of priority from X to Y. Input The INPUT keyword is used to tell which argument your module need in order to works. It's arguments are stored as a python array of strings. The available arguments are: * 'file' : Absolute path to the input binary to deal with. * 'online' : Boolean that indicate if the scan is online of offline. * 'id' : Identifier number for the scan. * 'None' : No input arguments. Their is no order for thoses arguments, but keep it aligned with your main() declaration. If your main() function is declared as such: def main(input_file, is_online): The input tag must follow the same order: INPUT = ['file', 'online'] Output If your module require to store something to the filesystem as an output, you can specify it in the OUTPUT tag. It's arguments are stored as a python array of strings. You can specify relative hardocded path to the output tag, but keep in mind that the translation of the following keywords is automatically done, in order to adda layer of absraction from the module's point of view. 'base' : Path to the base of the scan filesystem 'sha1hash' : SHA1sum of the given binary 'sha256hash' : SHA256sum of the given binary 'md5hash' MD5sum of the given binary 'id' : ID of the scan 'name' : Original name of the given file For instance, the following output tag is valid, and will be converted to the right absolute path at runtime: OUTPUT = ['/base/sha256hash/id/md5hash.disass'] Returns If your module returns a specific information that need to be taken in count by the next modules, you can specify it with the RETURNS tag. It's arguments are stored as a python array of strings. Return code In order to raise custom return code and errors for your module, use this tag to specify what are the available return code of your script, and what they mean. This is stored as a Python dictionnary, with an integer key and a string value (return code id : description). The following return code range need to be used when writting a module: * from 00 to 09 : General state (used to avoid 'None' as a return value. Keep tracks of the module's state) * from 10 to 19 : Critical errors (failure condition: will immediately stop any further modules) * from 21 to 50 : Conditionnal errors (will shortcut some specific modules) Here is a snippet of already implemented return code: 0: 'Module ended without errors' 1: 'Module ended with errors' 2: 'Scan succefull' 10: 'File not found' 11: 'WinAPI references not found (deprecated)' 12: 'Not a valid PE file' 13: 'Network error' 14: 'Metadenfender does not have an entry for the submited sample' 21: 'Is a dot net executable' 22: 'Is malformated' 23: 'Scanned binary may be packed' 24: 'Scanned binary does not seems packed' Module template The following template can be used to write a new module: # Module header PRIORITY = 55 INPUT = ['file'] OUTPUT = ['None'] RETURNS = ['None'] RETURN_CODE = { 99 : \"Template module\", 100 : \"Something else\" } # Main code def main(path): if do_something(path): return 99 return 100 # For standalone execution of the script if __name__ == \"__main__\": parser = ArgumentParser(description='Module Name') parser.add_argument('-i', '--input', help='PE File to scan', metavar='input-file.exe', required=True) parser.add_argument('-l', '--log', help='Log level. Default=INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], required=False, default='INFO') args = parser.parse_args() path = args.input main(args.online, args.input)","title":"4. Module Development Guide"},{"location":"II.%20Development%20Procedures/4.%20Module%20Development%20Guide/#module-development-guide","text":"This short guide aim to help developers to write and integrate new functionalities in the Cerberus-Sandbox. In Cerberus-Sandbox, every functionnal piece of code can be describe as a module. A module can work as a standalone tool, but can also be easily integrated in the general workflow of the sandbox. The following will give you an insight on what's a module, how to make it works, and how to integrate it into Cerberus.","title":"Module Development Guide"},{"location":"II.%20Development%20Procedures/4.%20Module%20Development%20Guide/#general-layout","text":"Modules are stored as Python scripts under cerberus/core/static_engine/Modules/ . By keeping everything in a modular architecture, we aim to encourage developers to add custom functionnalities that match their needs, without having to understand the whole code-base of the project. It is also easier for the inital development stage and for further official upgrades, as we don't need to deal with already existing works and only focus on the core of the functionnalities that we want to add. It also gave the ability to easily write new connector with already-existing products, wich is something we want to push forward.","title":"General Layout"},{"location":"II.%20Development%20Procedures/4.%20Module%20Development%20Guide/#orchestrator","text":"The script cerberus/core/static_engine/main.py is in charge of the orchestration of the modules. This script is the one that gather, parse and execute modules. If a new Python script is simply dropped in the modules folder, without any tweaks to make it works with the so called orchestrator, it will simply be ignored at runtime. If your module is malformated, this is the script that will tell you why and where. Don't make modification to the core of this script in order to match your needs ! If you have to, it means that your module is not well-written.","title":"Orchestrator"},{"location":"II.%20Development%20Procedures/4.%20Module%20Development%20Guide/#module-layout","text":"Each module must have a header with the following items: PRIORITY = INPUT = [''] OUTPUT = [''] RETURNS = [''] RETURN_CODE = { 0 : \"Something\" }","title":"Module layout"},{"location":"II.%20Development%20Procedures/4.%20Module%20Development%20Guide/#priority","text":"The PRIORITY keyword is used to tell in which order the modules must be launched. Priority numbers don't have to be contiguous. If you want to write several modules, you can reserve an arbitray range of priority from X to Y.","title":"Priority"},{"location":"II.%20Development%20Procedures/4.%20Module%20Development%20Guide/#input","text":"The INPUT keyword is used to tell which argument your module need in order to works. It's arguments are stored as a python array of strings. The available arguments are: * 'file' : Absolute path to the input binary to deal with. * 'online' : Boolean that indicate if the scan is online of offline. * 'id' : Identifier number for the scan. * 'None' : No input arguments. Their is no order for thoses arguments, but keep it aligned with your main() declaration. If your main() function is declared as such: def main(input_file, is_online): The input tag must follow the same order: INPUT = ['file', 'online']","title":"Input"},{"location":"II.%20Development%20Procedures/4.%20Module%20Development%20Guide/#output","text":"If your module require to store something to the filesystem as an output, you can specify it in the OUTPUT tag. It's arguments are stored as a python array of strings. You can specify relative hardocded path to the output tag, but keep in mind that the translation of the following keywords is automatically done, in order to adda layer of absraction from the module's point of view. 'base' : Path to the base of the scan filesystem 'sha1hash' : SHA1sum of the given binary 'sha256hash' : SHA256sum of the given binary 'md5hash' MD5sum of the given binary 'id' : ID of the scan 'name' : Original name of the given file For instance, the following output tag is valid, and will be converted to the right absolute path at runtime: OUTPUT = ['/base/sha256hash/id/md5hash.disass']","title":"Output"},{"location":"II.%20Development%20Procedures/4.%20Module%20Development%20Guide/#returns","text":"If your module returns a specific information that need to be taken in count by the next modules, you can specify it with the RETURNS tag. It's arguments are stored as a python array of strings.","title":"Returns"},{"location":"II.%20Development%20Procedures/4.%20Module%20Development%20Guide/#return-code","text":"In order to raise custom return code and errors for your module, use this tag to specify what are the available return code of your script, and what they mean. This is stored as a Python dictionnary, with an integer key and a string value (return code id : description). The following return code range need to be used when writting a module: * from 00 to 09 : General state (used to avoid 'None' as a return value. Keep tracks of the module's state) * from 10 to 19 : Critical errors (failure condition: will immediately stop any further modules) * from 21 to 50 : Conditionnal errors (will shortcut some specific modules) Here is a snippet of already implemented return code: 0: 'Module ended without errors' 1: 'Module ended with errors' 2: 'Scan succefull' 10: 'File not found' 11: 'WinAPI references not found (deprecated)' 12: 'Not a valid PE file' 13: 'Network error' 14: 'Metadenfender does not have an entry for the submited sample' 21: 'Is a dot net executable' 22: 'Is malformated' 23: 'Scanned binary may be packed' 24: 'Scanned binary does not seems packed'","title":"Return code"},{"location":"II.%20Development%20Procedures/4.%20Module%20Development%20Guide/#module-template","text":"The following template can be used to write a new module: # Module header PRIORITY = 55 INPUT = ['file'] OUTPUT = ['None'] RETURNS = ['None'] RETURN_CODE = { 99 : \"Template module\", 100 : \"Something else\" } # Main code def main(path): if do_something(path): return 99 return 100 # For standalone execution of the script if __name__ == \"__main__\": parser = ArgumentParser(description='Module Name') parser.add_argument('-i', '--input', help='PE File to scan', metavar='input-file.exe', required=True) parser.add_argument('-l', '--log', help='Log level. Default=INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'], required=False, default='INFO') args = parser.parse_args() path = args.input main(args.online, args.input)","title":"Module template"},{"location":"III.%20Installation/1.%20Foreword/","text":"Foreword This chapter explains how to install Cerberus. The recommended setup is GNU/Linux (Debian / Ubuntu or RHEL). Cerberus host wasn't test on Mac OS X and Windows X yet. There aren't any recommended tips for guests, futhermore with Cerberus you can deploy any Operating Systems you want. Nervetheless you must provide an ISO image file. Requirements a. Packages b. Virtual Environment c. Initialize Database d. Redis Connectivity e. Setting Up .env file f. Nuxt g. Load References and Rules NIDS a. Debian Installation b. Suricata Installation c. Suricata Configuration d. Suricata Rules e. TCPDump Installation f. Installing a Fake Internet with INetSIM and PolarProxy VMI","title":"1. Foreword"},{"location":"III.%20Installation/1.%20Foreword/#foreword","text":"This chapter explains how to install Cerberus. The recommended setup is GNU/Linux (Debian / Ubuntu or RHEL). Cerberus host wasn't test on Mac OS X and Windows X yet. There aren't any recommended tips for guests, futhermore with Cerberus you can deploy any Operating Systems you want. Nervetheless you must provide an ISO image file. Requirements a. Packages b. Virtual Environment c. Initialize Database d. Redis Connectivity e. Setting Up .env file f. Nuxt g. Load References and Rules NIDS a. Debian Installation b. Suricata Installation c. Suricata Configuration d. Suricata Rules e. TCPDump Installation f. Installing a Fake Internet with INetSIM and PolarProxy VMI","title":"Foreword"},{"location":"III.%20Installation/2.%20Requirements/","text":"Requirements Packages Before installing Cerberus, it is necessary to have some preinstalled packages. # CentOS distribution: $ yum install mariadb mariadb-devel mariadb-server redis python-pip python-devel ssdeep ssdeep-devel libfuzzy-dev nodejs # Ubuntu distribution: $ sudo apt install mariadb-server redis python-devel ssdeep libfuzzy-dev nodejs python3-pip default-libmysqlclient-dev build-essential Virtual Environment Create a Python virtual environment and install all the needed packages: $ mkvirtualenv cerberus $ pip install -r requirements.txt Initialize Database $ sudo mysql_secure_installation > Set root password? [Y/n] y > New password: > Re-enter new password: > Remove anonymous users? [Y/n] y > Disallow root login remotely? [Y/n] y > Remove test database and access to it? [Y/n] n > Reload privilege tables now? [Y/n] y $ sudo mysql -u root -p > CREATE DATABASE cerberusdb CHARACTER SET utf8; > CREATE USER `login`@localhost IDENTIFIED BY '`password`'; > GRANT ALL PRIVILEGES ON cerberusdb.* TO `login`@localhost; Redis connectivity verification In order to verify if Redis is up and running, execute the following command: $ redis-cli ping If the command returns PONG, Redis is up and running. Setting up the .env file The .env file allow to configure Django without sending all the confidential data (like secret key or databases credentials ) to Git. First, copy the .env.example template file: $ cp cerberus/cerberus/.env.example cerberus/cerberus/.env Then, fill the .env file by following the steps below. Fill the SECRET_KEY variable, by running this command: python -c 'from django.core.management.utils import get_random_secret_key; print(get_random_secret_key())' Fill in the MARIADB_URL variable as follow: MARIADB_URL=mysql://<login>:<password>@<db_host>:3306/<db_name> And you can fill the three API Key - UNPACME_API : UnpacMe - VT_API : VirusTotal - MD_API : MetaDefender Once the .env file is successfully filled, we can create the database architecture and start the server by running the following commands: $ cd cerberus/ $ python manage.py makemigrations $ python manage.py migrate $ python manage.py runserver To load all the necessary fixtures (like WinAPI, DLL or Section references) in the database, run the following command: $ python manage.py loaddata */fixtures/*.json.bz2 The API is at the following address: http://localhost:8000 You can create a user by running the following command: $ python manage.py createsuperuser You can access the admin panel with the /admin/ endpoint. Nuxt First, you need to install all the node packages: $ cd cerberus/web $ npm install Once the installation is complete, you can start the nuxt server and access it at the following address: http://localhost:3000 $ npm run dev Load references and rules Cerberus allows to load DLL, Windows API and Sections reference, YARA rules and IDS rules. In order to facilitate the import of a batch of data, it can create all these references by using CSV files. Here are the cURL command that allows to loads these CSV files in the databases : # Windows API references $ curl -F \"csv=@./winapi_9k.csv\" -X POST http://localhost:8000/api/winapi/ # DLL references $ curl -F \"csv=@./dll.csv\" -X POST http://localhost:8000/api/dll/ # Section references $ curl -F \"csv=@./section.csv\" -X POST http://localhost:8000/api/section/ # YARA rules $ curl -F \"csv=@./yara_sources.csv\" -X POST http://localhost:8000/api/yara/ # IDS rules $ curl -F \"csv=@./ids_sources.csv\" -X POST http://localhost:8000/api/ids/ If you want to dump the section, or rules sources databases content in CSV you can do it by using the following cURL command: # Replace <name> by winapi, dll, section, yara or ids $ curl -X GET http://localhost:8000/api/<name>/extract/","title":"2. Requirements"},{"location":"III.%20Installation/2.%20Requirements/#requirements","text":"","title":"Requirements"},{"location":"III.%20Installation/2.%20Requirements/#packages","text":"Before installing Cerberus, it is necessary to have some preinstalled packages. # CentOS distribution: $ yum install mariadb mariadb-devel mariadb-server redis python-pip python-devel ssdeep ssdeep-devel libfuzzy-dev nodejs # Ubuntu distribution: $ sudo apt install mariadb-server redis python-devel ssdeep libfuzzy-dev nodejs python3-pip default-libmysqlclient-dev build-essential","title":"Packages"},{"location":"III.%20Installation/2.%20Requirements/#virtual-environment","text":"Create a Python virtual environment and install all the needed packages: $ mkvirtualenv cerberus $ pip install -r requirements.txt","title":"Virtual Environment"},{"location":"III.%20Installation/2.%20Requirements/#initialize-database","text":"$ sudo mysql_secure_installation > Set root password? [Y/n] y > New password: > Re-enter new password: > Remove anonymous users? [Y/n] y > Disallow root login remotely? [Y/n] y > Remove test database and access to it? [Y/n] n > Reload privilege tables now? [Y/n] y $ sudo mysql -u root -p > CREATE DATABASE cerberusdb CHARACTER SET utf8; > CREATE USER `login`@localhost IDENTIFIED BY '`password`'; > GRANT ALL PRIVILEGES ON cerberusdb.* TO `login`@localhost;","title":"Initialize Database"},{"location":"III.%20Installation/2.%20Requirements/#redis-connectivity-verification","text":"In order to verify if Redis is up and running, execute the following command: $ redis-cli ping If the command returns PONG, Redis is up and running.","title":"Redis connectivity verification"},{"location":"III.%20Installation/2.%20Requirements/#setting-up-the-env-file","text":"The .env file allow to configure Django without sending all the confidential data (like secret key or databases credentials ) to Git. First, copy the .env.example template file: $ cp cerberus/cerberus/.env.example cerberus/cerberus/.env Then, fill the .env file by following the steps below. Fill the SECRET_KEY variable, by running this command: python -c 'from django.core.management.utils import get_random_secret_key; print(get_random_secret_key())' Fill in the MARIADB_URL variable as follow: MARIADB_URL=mysql://<login>:<password>@<db_host>:3306/<db_name> And you can fill the three API Key - UNPACME_API : UnpacMe - VT_API : VirusTotal - MD_API : MetaDefender Once the .env file is successfully filled, we can create the database architecture and start the server by running the following commands: $ cd cerberus/ $ python manage.py makemigrations $ python manage.py migrate $ python manage.py runserver To load all the necessary fixtures (like WinAPI, DLL or Section references) in the database, run the following command: $ python manage.py loaddata */fixtures/*.json.bz2 The API is at the following address: http://localhost:8000 You can create a user by running the following command: $ python manage.py createsuperuser You can access the admin panel with the /admin/ endpoint.","title":"Setting up the .env file"},{"location":"III.%20Installation/2.%20Requirements/#nuxt","text":"First, you need to install all the node packages: $ cd cerberus/web $ npm install Once the installation is complete, you can start the nuxt server and access it at the following address: http://localhost:3000 $ npm run dev","title":"Nuxt"},{"location":"III.%20Installation/2.%20Requirements/#load-references-and-rules","text":"Cerberus allows to load DLL, Windows API and Sections reference, YARA rules and IDS rules. In order to facilitate the import of a batch of data, it can create all these references by using CSV files. Here are the cURL command that allows to loads these CSV files in the databases : # Windows API references $ curl -F \"csv=@./winapi_9k.csv\" -X POST http://localhost:8000/api/winapi/ # DLL references $ curl -F \"csv=@./dll.csv\" -X POST http://localhost:8000/api/dll/ # Section references $ curl -F \"csv=@./section.csv\" -X POST http://localhost:8000/api/section/ # YARA rules $ curl -F \"csv=@./yara_sources.csv\" -X POST http://localhost:8000/api/yara/ # IDS rules $ curl -F \"csv=@./ids_sources.csv\" -X POST http://localhost:8000/api/ids/ If you want to dump the section, or rules sources databases content in CSV you can do it by using the following cURL command: # Replace <name> by winapi, dll, section, yara or ids $ curl -X GET http://localhost:8000/api/<name>/extract/","title":"Load references and rules"},{"location":"III.%20Installation/3.%20NIDS/","text":"NIDS Before installing an NIDS, you need to prepare your distribution. In our case, we are going to use Debian. Debian Installation For this installation, I choose the debian-10.6.0-amd64-netinst.iso . During the installation, untick debian desktop environment. Suricata installation Compilation chain installation In order to compile Suricata and its dependencies, it is necessary to install on the system, a compilation chain Lexical and syntax analysers GNU Bison : Compiler compiler in charge of semantic and syntactic analysis. Flex : Lexical pattern analyser. $ sudo apt install flex bison Autotools GNU Make : Provides help with compiling and linking by creating dependency installation description files called makefiles. GNU AutoMake : Allows the generation of a makefile from a higher level description. GNU AutoConf : Allows the generation of a shell script to configure the \"co nfigure\" development environment from programs based on the GNU M4 preprocessor. GNU LibTool : Used with AutoConf and AutoMake to simplify the compilation process. GNU AutoGen : Provides a similar approach to Flex in makefile generation. $ sudo apt install make automake autoconf libtool autogen m4 Compiler GNU BinUtils : Set of tools for the creation and management of binary programs and assembler sources. GNU Debugger : Provides a large set of tools for tracing or altering the execution of a program. GNU C++ : Compiler for C++. $ sudo apt install binutils gcc g++ gdb build-essential Installation of the dependencies To work, Suricata is mainly based on these libraries: LibPCRE : Provides functions for PCRE (\"Perl Compatible Regular Expressions\") based regular expression management. LibPcap : Provides functions for capturing network traffic. LibNet : Provides low-level network interaction functions. LibYaml : Provides data processing functions using the YAML form standard. LibNetFilter : Provides interaction functions with the kernel firewall. Zlib : Provides functions for compressing / decompressing data ; LibJansson : Provides functions for data processing using theJSON (JavaScript Object Notation) format. apt install libpcre3 libpcre3-dbg libpcre3-dev libpcap-dev libnet1-dev libyaml-0-2 libyaml-dev libnetfilter-queue-dev zlib1g zlib1g-dev libmagic-dev libcap-ng-dev libjansson-dev liblz4-dev libnss3-dev pkg-config magic coccinelle cbindgen rustc Suricata download To download and build Suricata, enter the following: $ wget http://www.openinfosecfoundation.org/download/suricata-6.0.0.tar.gz $ tar -xvzf suricata-6.0.0.tar.gz $ cd suricata-6.0.0 Suricata compilation To compile and install the program, you have to continue with the next commands: $ ./configure $ make $ make install To make sure the existing list with libraries will be updated with the new library, enter: $ ldconfig Suricata configuration Creation of configuration files Creation of the event log storage directory : $ mkdir /var/log/suricata Creation of rules storage directory : $ mkdir -p /etc/suricata/rules Creating configuration files : $ cp {suricata.yaml,etc/classification.config,etc/reference.config} /etc/suricata $ touch /etc/suricata/threshold.config Modification of the configuration file suricata.yaml : $ vi /etc/suricata/suricata.yaml Modification of local variables in Suricata : HOME_NET: \"192.168.25.0/24\" Deactivation of rules allowing alerts to be sent to the SIEM : # alert output to prelude (http://www.prelude-technologies.com/) only # available if Suricata has been compiled with --enable-prelude # - alert-prelude: # enabled: no # profile: suricata # log-packet-content: no # log-packet-header: yes Configuration of logging flows : # Define your logging outputs. If none are defined, or they are all # disabled you will get the default - console output. outputs: - console: enabled: yes - file: enabled: yes filename: /var/log/suricata/suricata.log Suricata rules First you have to install suricata-update : sudo apt install suricata-update If upgrading from an older version of Suricata, or running a development version that may not be bundled with Suricata-Update, you will have to check that your suricata.yaml is configured for Suricata-Update. The main difference is the default-rule-path which is /var/lib/suricata/rules when using Suricata-Update. You will want to update your suricata.yaml to have the following : default-rule-path: /var/lib/suricata/rules rule-files: - suricata.rules Discover Other Available Rule Sources First update the rule source index with the update-sources command, for example: $ suricata-update update-sources Then list the sources from the index. Example : $ suricata-update list-sources Now enable the ptresearch/attackdetection ruleset : $ suricata-update enable-source ptresearch/attackdetection And update your rules again: $ suricata-update Run suricata $ suricata -c /etc/suricata/suricata.yaml -i ens33 Tcpdump installation Tcpdump is a data-network packet analyzer computer program that runs under a command line interface. It allows the user to display TCP/IP and other packets being transmitted or received over a network to which the computer is attached. We are going to use it in order to capture the traffic between the malware and internet or fake internet. First, we must install the packet tcpdump : $ apt install tcp-dump If your $PATH is empty, You may set the PATH variable with this command : $ export PATH=\"/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin\" -w capture_file -> pcap -i [network interface] $ tcpdump -i [networkinterface] -w capture_file Installing a Fake Internet with INetSim and PolarProxy Inetsim is a software that simulates common internet services like HTTP, SMTP, DNS, FTP, IRC. This software is useful when analysing the network behavior of malware without connecting them to Internet. PolarProxy is a transparent SSL/TLS proxy. He is primarily designed to intercept and decrypt TLS encrypted traffic from malware. PolarProxy decrypts and re-encrypts TLS traffic, while also saving the decrypted traffic in a PCAP file that can be loaded into Wireshark or an intrusion detection system (IDS). INetSim Installation To install INetSim using apt, add the INetSim Debian Archive repository to your apt sources: $ echo \"deb http://www.inetsim.org/debian/ binary/\" > /etc/apt/sources.list.d/inetsim.list To access the Debian package sources, also add: bash $ echo \"deb-src http://www.inetsim.org/debian/ source/\" >> /etc/apt/sources.list.d/inetsim.list To allow apt to verify the digital signature on the INetSim Debian Archive's Release file, add the INetSim Archive Signing Key to the apt trusted keys: $ wget -O - https://www.inetsim.org/inetsim-archive-signing-key.asc | apt-key add - After installing the key, update the cache of available packages: $ apt update Finally, install INetSim: $ apt install inetsim Configuration By default INetSim listens on 127.0.0.1, for change this you need to un-commenting and editing the service_bind-address variable in /etc/inetsim/inetsim.conf. service_bind_address 192.168.53.19 Also configure INetSim's fake DNS server to resolve all domain names to the IP of INetSim with the dns_default_ip setting: dns_default_ip 192.168.53.19 Finally, disable the start_service https and start_service smtps lines, because these services will be replaced with PolarProxy: start_service dns start_service http #start_service https start_service smtp #start_service smtps Restart the INetSim service after changing the config. sudo systemctl restart inetsim.service Verify that you can access INetSim's HTTP server with curl: curl http://192.168.53.19 PolarProxy installation sudo mkdir /var/log/PolarProxy mkdir ~/PolarProxy cd ~/PolarProxy/ curl https://www.netresec.com/?download=PolarProxy | tar -xzvf - exit sudo cp /home/proxyuser/PolarProxy/PolarProxy.service /etc/systemd/system/PolarProxy.service We will need to modify the PolarProxy service config file a bit before we start it. Edit the ExecStart setting in /etc/systemd/system/PolarProxy.service to configure PolarProxy to terminate the TLS encryption for HTTPS and SMTPS (implicitly encrypted email submission). The HTTPS traffic should be redirected to INetSim's web server on tcp/80 and the SMTPS to tcp/25. ExecStart=/home/proxyuser/PolarProxy/PolarProxy -v -p 10443,80,80 -p 10465,25,25 -x /var/log/PolarProxy/polarproxy.cer -f /var/log/PolarProxy/proxyflows.log -o /var/log/PolarProxy/ --certhttp 10080 --terminate --connect 192.168.53.19 --nosni nosni.inetsim.org Here's a break-down of the arguments sent to PolarProxy through the ExecStart setting above: -v : verbose output in syslog (not required) -p 10443,80,80 : listen for TLS connections on tcp/10443, save decrypted traffic in PCAP as tcp/80, forward traffic to tcp/80 -p 10465,25,25 : listen for TLS connections on tcp/10465, save decrypted traffic in PCAP as tcp/25, forward traffic to tcp/25 -x /var/log/PolarProxy/polarproxy.cer : Save certificate to be imported to clients in /var/log/PolarProxy/polarproxy.cer (not required) -f /var/log/PolarProxy/proxyflows.log : Log flow meta data in /var/log/PolarProxy/proxyflows.log (not required) -o /var/log/PolarProxy/ : Save PCAP files with decrypted traffic in /var/log/PolarProxy/ --certhttp 10080 : Make the X.509 certificate available to clients over http on tcp/10080 --terminate : Run PolarProxy as a TLS termination proxy, i.e. data forwarded from the proxy is decrypted --connect 192.168.53.19 : forward all connections to the IP of INetSim --nosni nosni.inetsim.org : Accept incoming TLS connections without SNI, behave as if server name was \"nosni.inetsim.org\". Finally, start the PolarProxy systemd service: $ sudo systemctl enable PolarProxy.service $ sudo systemctl start PolarProxy.service Verify that you can reach INetSim through PolarProxy's TLS termination proxy using curl: $ curl --insecure --connect-to example.com:443:192.168.53.19:10443 https://example.com Do the same thing again, but also verify the certificate against PolarProxy's root CA this time. The root certificate is downloaded from PolarProxy via the HTTP service running on tcp/10080 and then converted from DER to PEM format using openssl, so that it can be used with curl's \"--cacert\" option. $ curl http://192.168.53.19:10080/polarproxy.cer > polarproxy.cer $ openssl x509 -inform DER -in polarproxy.cer -out polarproxy-pem.crt $ curl --cacert polarproxy-pem.crt --connect-to example.com:443:192.168.53.19:10443 https://example.com Now let's set up routing to forward all HTTPS traffic to PolarProxy's service on tcp/10443 and SMTPS traffic to tcp/10465. I'm also adding a firewall rule to redirect ALL other incoming traffic to INetSim, regardless of which IP it is destined to, with the final REDIRECT rule. Make sure to replace \"enp0s8\" with the name of your interface. $ sudo iptables -t nat -A PREROUTING -i enp0s8 -p tcp --dport 443 -j REDIRECT --to 10443 $ sudo iptables -t nat -A PREROUTING -i enp0s8 -p tcp --dport 465 -j REDIRECT --to 10465 $ sudo iptables -t nat -A PREROUTING -i enp0s8 -j REDIRECT To check the iptables rules : $ sudo iptables -t nat -L Verify that the iptables port redirection rule is working from another machine connected to the offline 192.168.53.0/24 network: $ curl --insecure --resolve example.com:443:192.168.53.19 https://example.com $ curl --insecure --resolve example.com:465:192.168.53.19 smtps://example.com It is now time to save the firewall rules, so that they will survive reboots. $ sudo apt-get install iptables-persistent Sources https://www.netresec.com/?page=Blog&month=2019-12& post=Installing-a-Fake-Internet-with-INetSim-and-PolarProxy https://github.com/catmin/inetsim/tree/master/data/http/fakefiles","title":"3. NIDS"},{"location":"III.%20Installation/3.%20NIDS/#nids","text":"Before installing an NIDS, you need to prepare your distribution. In our case, we are going to use Debian.","title":"NIDS"},{"location":"III.%20Installation/3.%20NIDS/#debian-installation","text":"For this installation, I choose the debian-10.6.0-amd64-netinst.iso . During the installation, untick debian desktop environment.","title":"Debian Installation"},{"location":"III.%20Installation/3.%20NIDS/#suricata-installation","text":"","title":"Suricata installation"},{"location":"III.%20Installation/3.%20NIDS/#compilation-chain-installation","text":"In order to compile Suricata and its dependencies, it is necessary to install on the system, a compilation chain Lexical and syntax analysers GNU Bison : Compiler compiler in charge of semantic and syntactic analysis. Flex : Lexical pattern analyser. $ sudo apt install flex bison Autotools GNU Make : Provides help with compiling and linking by creating dependency installation description files called makefiles. GNU AutoMake : Allows the generation of a makefile from a higher level description. GNU AutoConf : Allows the generation of a shell script to configure the \"co nfigure\" development environment from programs based on the GNU M4 preprocessor. GNU LibTool : Used with AutoConf and AutoMake to simplify the compilation process. GNU AutoGen : Provides a similar approach to Flex in makefile generation. $ sudo apt install make automake autoconf libtool autogen m4 Compiler GNU BinUtils : Set of tools for the creation and management of binary programs and assembler sources. GNU Debugger : Provides a large set of tools for tracing or altering the execution of a program. GNU C++ : Compiler for C++. $ sudo apt install binutils gcc g++ gdb build-essential Installation of the dependencies To work, Suricata is mainly based on these libraries: LibPCRE : Provides functions for PCRE (\"Perl Compatible Regular Expressions\") based regular expression management. LibPcap : Provides functions for capturing network traffic. LibNet : Provides low-level network interaction functions. LibYaml : Provides data processing functions using the YAML form standard. LibNetFilter : Provides interaction functions with the kernel firewall. Zlib : Provides functions for compressing / decompressing data ; LibJansson : Provides functions for data processing using theJSON (JavaScript Object Notation) format. apt install libpcre3 libpcre3-dbg libpcre3-dev libpcap-dev libnet1-dev libyaml-0-2 libyaml-dev libnetfilter-queue-dev zlib1g zlib1g-dev libmagic-dev libcap-ng-dev libjansson-dev liblz4-dev libnss3-dev pkg-config magic coccinelle cbindgen rustc","title":"Compilation chain installation"},{"location":"III.%20Installation/3.%20NIDS/#suricata-download","text":"To download and build Suricata, enter the following: $ wget http://www.openinfosecfoundation.org/download/suricata-6.0.0.tar.gz $ tar -xvzf suricata-6.0.0.tar.gz $ cd suricata-6.0.0","title":"Suricata download"},{"location":"III.%20Installation/3.%20NIDS/#suricata-compilation","text":"To compile and install the program, you have to continue with the next commands: $ ./configure $ make $ make install To make sure the existing list with libraries will be updated with the new library, enter: $ ldconfig","title":"Suricata compilation"},{"location":"III.%20Installation/3.%20NIDS/#suricata-configuration","text":"Creation of configuration files Creation of the event log storage directory : $ mkdir /var/log/suricata Creation of rules storage directory : $ mkdir -p /etc/suricata/rules Creating configuration files : $ cp {suricata.yaml,etc/classification.config,etc/reference.config} /etc/suricata $ touch /etc/suricata/threshold.config Modification of the configuration file suricata.yaml : $ vi /etc/suricata/suricata.yaml Modification of local variables in Suricata : HOME_NET: \"192.168.25.0/24\" Deactivation of rules allowing alerts to be sent to the SIEM : # alert output to prelude (http://www.prelude-technologies.com/) only # available if Suricata has been compiled with --enable-prelude # - alert-prelude: # enabled: no # profile: suricata # log-packet-content: no # log-packet-header: yes Configuration of logging flows : # Define your logging outputs. If none are defined, or they are all # disabled you will get the default - console output. outputs: - console: enabled: yes - file: enabled: yes filename: /var/log/suricata/suricata.log","title":"Suricata configuration"},{"location":"III.%20Installation/3.%20NIDS/#suricata-rules","text":"First you have to install suricata-update : sudo apt install suricata-update If upgrading from an older version of Suricata, or running a development version that may not be bundled with Suricata-Update, you will have to check that your suricata.yaml is configured for Suricata-Update. The main difference is the default-rule-path which is /var/lib/suricata/rules when using Suricata-Update. You will want to update your suricata.yaml to have the following : default-rule-path: /var/lib/suricata/rules rule-files: - suricata.rules Discover Other Available Rule Sources First update the rule source index with the update-sources command, for example: $ suricata-update update-sources Then list the sources from the index. Example : $ suricata-update list-sources Now enable the ptresearch/attackdetection ruleset : $ suricata-update enable-source ptresearch/attackdetection And update your rules again: $ suricata-update","title":"Suricata rules"},{"location":"III.%20Installation/3.%20NIDS/#run-suricata","text":"$ suricata -c /etc/suricata/suricata.yaml -i ens33","title":"Run suricata"},{"location":"III.%20Installation/3.%20NIDS/#tcpdump-installation","text":"Tcpdump is a data-network packet analyzer computer program that runs under a command line interface. It allows the user to display TCP/IP and other packets being transmitted or received over a network to which the computer is attached. We are going to use it in order to capture the traffic between the malware and internet or fake internet. First, we must install the packet tcpdump : $ apt install tcp-dump If your $PATH is empty, You may set the PATH variable with this command : $ export PATH=\"/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin\" -w capture_file -> pcap -i [network interface] $ tcpdump -i [networkinterface] -w capture_file","title":"Tcpdump installation"},{"location":"III.%20Installation/3.%20NIDS/#installing-a-fake-internet-with-inetsim-and-polarproxy","text":"Inetsim is a software that simulates common internet services like HTTP, SMTP, DNS, FTP, IRC. This software is useful when analysing the network behavior of malware without connecting them to Internet. PolarProxy is a transparent SSL/TLS proxy. He is primarily designed to intercept and decrypt TLS encrypted traffic from malware. PolarProxy decrypts and re-encrypts TLS traffic, while also saving the decrypted traffic in a PCAP file that can be loaded into Wireshark or an intrusion detection system (IDS).","title":"Installing a Fake Internet with INetSim and PolarProxy"},{"location":"III.%20Installation/3.%20NIDS/#inetsim-installation","text":"To install INetSim using apt, add the INetSim Debian Archive repository to your apt sources: $ echo \"deb http://www.inetsim.org/debian/ binary/\" > /etc/apt/sources.list.d/inetsim.list To access the Debian package sources, also add: bash $ echo \"deb-src http://www.inetsim.org/debian/ source/\" >> /etc/apt/sources.list.d/inetsim.list To allow apt to verify the digital signature on the INetSim Debian Archive's Release file, add the INetSim Archive Signing Key to the apt trusted keys: $ wget -O - https://www.inetsim.org/inetsim-archive-signing-key.asc | apt-key add - After installing the key, update the cache of available packages: $ apt update Finally, install INetSim: $ apt install inetsim","title":"INetSim Installation"},{"location":"III.%20Installation/3.%20NIDS/#configuration","text":"By default INetSim listens on 127.0.0.1, for change this you need to un-commenting and editing the service_bind-address variable in /etc/inetsim/inetsim.conf. service_bind_address 192.168.53.19 Also configure INetSim's fake DNS server to resolve all domain names to the IP of INetSim with the dns_default_ip setting: dns_default_ip 192.168.53.19 Finally, disable the start_service https and start_service smtps lines, because these services will be replaced with PolarProxy: start_service dns start_service http #start_service https start_service smtp #start_service smtps Restart the INetSim service after changing the config. sudo systemctl restart inetsim.service Verify that you can access INetSim's HTTP server with curl: curl http://192.168.53.19","title":"Configuration"},{"location":"III.%20Installation/3.%20NIDS/#polarproxy-installation","text":"sudo mkdir /var/log/PolarProxy mkdir ~/PolarProxy cd ~/PolarProxy/ curl https://www.netresec.com/?download=PolarProxy | tar -xzvf - exit sudo cp /home/proxyuser/PolarProxy/PolarProxy.service /etc/systemd/system/PolarProxy.service We will need to modify the PolarProxy service config file a bit before we start it. Edit the ExecStart setting in /etc/systemd/system/PolarProxy.service to configure PolarProxy to terminate the TLS encryption for HTTPS and SMTPS (implicitly encrypted email submission). The HTTPS traffic should be redirected to INetSim's web server on tcp/80 and the SMTPS to tcp/25. ExecStart=/home/proxyuser/PolarProxy/PolarProxy -v -p 10443,80,80 -p 10465,25,25 -x /var/log/PolarProxy/polarproxy.cer -f /var/log/PolarProxy/proxyflows.log -o /var/log/PolarProxy/ --certhttp 10080 --terminate --connect 192.168.53.19 --nosni nosni.inetsim.org Here's a break-down of the arguments sent to PolarProxy through the ExecStart setting above: -v : verbose output in syslog (not required) -p 10443,80,80 : listen for TLS connections on tcp/10443, save decrypted traffic in PCAP as tcp/80, forward traffic to tcp/80 -p 10465,25,25 : listen for TLS connections on tcp/10465, save decrypted traffic in PCAP as tcp/25, forward traffic to tcp/25 -x /var/log/PolarProxy/polarproxy.cer : Save certificate to be imported to clients in /var/log/PolarProxy/polarproxy.cer (not required) -f /var/log/PolarProxy/proxyflows.log : Log flow meta data in /var/log/PolarProxy/proxyflows.log (not required) -o /var/log/PolarProxy/ : Save PCAP files with decrypted traffic in /var/log/PolarProxy/ --certhttp 10080 : Make the X.509 certificate available to clients over http on tcp/10080 --terminate : Run PolarProxy as a TLS termination proxy, i.e. data forwarded from the proxy is decrypted --connect 192.168.53.19 : forward all connections to the IP of INetSim --nosni nosni.inetsim.org : Accept incoming TLS connections without SNI, behave as if server name was \"nosni.inetsim.org\". Finally, start the PolarProxy systemd service: $ sudo systemctl enable PolarProxy.service $ sudo systemctl start PolarProxy.service Verify that you can reach INetSim through PolarProxy's TLS termination proxy using curl: $ curl --insecure --connect-to example.com:443:192.168.53.19:10443 https://example.com Do the same thing again, but also verify the certificate against PolarProxy's root CA this time. The root certificate is downloaded from PolarProxy via the HTTP service running on tcp/10080 and then converted from DER to PEM format using openssl, so that it can be used with curl's \"--cacert\" option. $ curl http://192.168.53.19:10080/polarproxy.cer > polarproxy.cer $ openssl x509 -inform DER -in polarproxy.cer -out polarproxy-pem.crt $ curl --cacert polarproxy-pem.crt --connect-to example.com:443:192.168.53.19:10443 https://example.com Now let's set up routing to forward all HTTPS traffic to PolarProxy's service on tcp/10443 and SMTPS traffic to tcp/10465. I'm also adding a firewall rule to redirect ALL other incoming traffic to INetSim, regardless of which IP it is destined to, with the final REDIRECT rule. Make sure to replace \"enp0s8\" with the name of your interface. $ sudo iptables -t nat -A PREROUTING -i enp0s8 -p tcp --dport 443 -j REDIRECT --to 10443 $ sudo iptables -t nat -A PREROUTING -i enp0s8 -p tcp --dport 465 -j REDIRECT --to 10465 $ sudo iptables -t nat -A PREROUTING -i enp0s8 -j REDIRECT To check the iptables rules : $ sudo iptables -t nat -L Verify that the iptables port redirection rule is working from another machine connected to the offline 192.168.53.0/24 network: $ curl --insecure --resolve example.com:443:192.168.53.19 https://example.com $ curl --insecure --resolve example.com:465:192.168.53.19 smtps://example.com It is now time to save the firewall rules, so that they will survive reboots. $ sudo apt-get install iptables-persistent","title":"PolarProxy installation"},{"location":"III.%20Installation/3.%20NIDS/#sources","text":"https://www.netresec.com/?page=Blog&month=2019-12& post=Installing-a-Fake-Internet-with-INetSim-and-PolarProxy https://github.com/catmin/inetsim/tree/master/data/http/fakefiles","title":"Sources"},{"location":"III.%20Installation/4.%20VMI/","text":"VMI Virtual Memory Introspection TODO https://github.com/hvmi/hvmi https://github.com/fireeye/rvmi https://github.com/KVM-VMI https://drakvuf.com/","title":"4. VMI"},{"location":"III.%20Installation/4.%20VMI/#vmi","text":"Virtual Memory Introspection TODO https://github.com/hvmi/hvmi https://github.com/fireeye/rvmi https://github.com/KVM-VMI https://drakvuf.com/","title":"VMI"},{"location":"IV.%20Internal%20Design/","text":"","title":"Index"}]}